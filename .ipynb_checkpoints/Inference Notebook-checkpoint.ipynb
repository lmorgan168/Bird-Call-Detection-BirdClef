{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "choice-budget",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import ast\n",
    "import random\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "import librosa\n",
    "import librosa.display\n",
    "import IPython.display as ipd\n",
    "import albumentations as A\n",
    "import albumentations.pytorch.transforms as T\n",
    "import timm\n",
    "from pathlib import Path\n",
    "\n",
    "from torchlibrosa.augmentation import SpecAugmentation\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torchvision import models\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from torchaudio import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ruled-moscow",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta = pd.read_csv(\"C:/UCI MDS Spring Quarter 2022/CS 274P/Final Project/train_metadata.csv\")\n",
    "test = pd.read_csv(\"C:/UCI MDS Spring Quarter 2022/CS 274P/Final Project/test.csv\")\n",
    "taxonomy = pd.read_csv(\"C:/UCI MDS Spring Quarter 2022/CS 274P/Final Project/eBird_Taxonomy_v2021.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "major-pharmacology",
   "metadata": {},
   "outputs": [],
   "source": [
    "## seeding\n",
    "\n",
    "OUTPUT_DIR = f'./'\n",
    "if not os.path.exists(OUTPUT_DIR):\n",
    "    os.makedirs(OUTPUT_DIR)\n",
    "   \n",
    "\n",
    "### Seeding purposes    \n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)                                                     ## Seed RNG for custom operators\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)                              ## Set environment hash to seed\n",
    "    np.random.seed(seed)                                                  ## Seed RNG for libraries dependent on numpy\n",
    "    torch.manual_seed(seed)                                               ## set random seed for CPU AND GPU\n",
    "    torch.backends.cudnn.deterministic = False                            ## Benchmark=False makes CudNN select same algorithm, but the algorithm itself might be non-deterministic. So, we ensure the algorithm is deterministic\n",
    "    torch.backends.cudnn.benchmark = True                                 ## CudNN might select better algorithm on each run to improve performance, varies and makes it irreproducible\n",
    "    \n",
    "set_seed(config.seed)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wanted-upper",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_layer(layer):\n",
    "    \"\"\"Initialize a Linear or Convolutional layer. \"\"\"\n",
    "    nn.init.xavier_uniform_(layer.weight)\n",
    " \n",
    "    if hasattr(layer, 'bias'):\n",
    "        if layer.bias is not None:\n",
    "            layer.bias.data.fill_(0.)\n",
    "\n",
    "# batchnorm layer\n",
    "def init_bn(bn):\n",
    "    bn.bias.data.fill_(0.)\n",
    "    bn.weight.data.fill_(1.)\n",
    "\n",
    "\n",
    "def init_weights(model):\n",
    "    classname = model.__class__.__name__\n",
    "    if classname.find(\"Conv2d\") != -1:\n",
    "        nn.init.xavier_uniform_(model.weight, gain=np.sqrt(2))\n",
    "        model.bias.data.fill_(0)\n",
    "    elif classname.find(\"BatchNorm\") != -1:\n",
    "        model.weight.data.normal_(1.0, 0.02)\n",
    "        model.bias.data.fill_(0)\n",
    "    elif classname.find(\"GRU\") != -1:\n",
    "        for weight in model.parameters():\n",
    "            if len(weight.size()) > 1:\n",
    "                nn.init.orghogonal_(weight.data)\n",
    "    elif classname.find(\"Linear\") != -1:\n",
    "        model.weight.data.normal_(0, 0.01)\n",
    "        model.bias.data.zero_()\n",
    "        \n",
    "def do_mixup(x, mixup_lambda):\n",
    "    \"\"\"Mixup x of even indexes (0, 2, 4, ...) with x of odd indexes \n",
    "    (1, 3, 5, ...).\n",
    "    \"\"\"\n",
    "    out = (x[0 :: 2].transpose(0, -1) * mixup_lambda[0 :: 2] + \\\n",
    "        x[1 :: 2].transpose(0, -1) * mixup_lambda[1 :: 2]).transpose(0, -1)\n",
    "    return out\n",
    "\n",
    "def interpolate(x, ratio):\n",
    "    \"\"\"Interpolate data in time domain. This is used to compensate the \n",
    "    resolution reduction in downsampling of a CNN.\n",
    "    \"\"\"\n",
    "    (batch_size, time_steps, classes_num) = x.shape\n",
    "    upsampled = x[:, :, None, :].repeat(1, 1, ratio, 1)\n",
    "    upsampled = upsampled.reshape(batch_size, time_steps * ratio, classes_num)\n",
    "    return upsampled\n",
    "\n",
    "def pad_framewise_output(framewise_output, frames_num):\n",
    "    \"\"\"Pad framewise_output to the same length as input frames. The pad value \n",
    "    is the same as the value of the last frame.\n",
    "    \"\"\"\n",
    "    pad = framewise_output[:, -1 :, :].repeat(1, frames_num - framewise_output.shape[1], 1)\n",
    "    \"\"\"tensor for padding\"\"\"\n",
    "\n",
    "    output = torch.cat((framewise_output, pad), dim=1)\n",
    "    \"\"\"(batch_size, frames_num, classes_num)\"\"\"\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "# attention block for deep multiple instance learning\n",
    "class AttBlock(nn.Module):\n",
    "    def __init__(self, n_in, n_out, activation='linear'):\n",
    "        super(AttBlock, self).__init__()\n",
    "        \n",
    "        self.activation = activation\n",
    "        self.att = nn.Conv1d(in_channels=n_in, out_channels=n_out, kernel_size=1, stride=1, padding=0, bias=True)\n",
    "        self.cla = nn.Conv1d(in_channels=n_in, out_channels=n_out, kernel_size=1, stride=1, padding=0, bias=True)\n",
    "        \n",
    "        self.init_weights()\n",
    "        \n",
    "    def init_weights(self):\n",
    "        init_layer(self.att)\n",
    "        init_layer(self.cla)\n",
    "         \n",
    "    def forward(self, x):\n",
    "        # x: (n_samples, n_in, n_time)\n",
    "        norm_att = torch.softmax(torch.clamp(self.att(x), -10, 10), dim=-1)\n",
    "        cla = self.nonlinear_transform(self.cla(x))\n",
    "        x = torch.sum(norm_att * cla, dim=2)\n",
    "        return x, norm_att, cla\n",
    "\n",
    "    def nonlinear_transform(self, x):\n",
    "        if self.activation == 'linear':\n",
    "            return x\n",
    "        elif self.activation == 'sigmoid':\n",
    "            return torch.sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proper-easter",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fine tuning a pretrained model.\n",
    "\n",
    "class BirdCLEFPretrain(nn.Module):\n",
    "  def __init__(self, mel_bins: int, pretrained: bool, num_classes : int, in_channels= 1):\n",
    "    \n",
    "    # Why num_classes = 24 in EX005??\n",
    "    super().__init__() \n",
    "\n",
    "    base_model = timm.create_model(\n",
    "          \"tf_efficientnet_b0_ns\", pretrained=True, in_chans=in_channels)\n",
    "    \n",
    "    \n",
    "    #base_model = torch.hub.load('harritaylor/torchvggish', 'vggish')\n",
    "\n",
    "    self.spec_augmenter = SpecAugmentation(\n",
    "        time_drop_width = 64,\n",
    "        time_stripes_num = 2,\n",
    "        freq_drop_width = 8,\n",
    "        freq_stripes_num = 2)\n",
    "\n",
    "    self.bn0 = nn.BatchNorm2d(mel_bins)\n",
    "\n",
    "    #conv1_weight = base_model.features[0].weight\n",
    "    #conv1_type = conv1_weight.dtype\n",
    "    #conv1_weight = conv1_weight.float()\n",
    "    #repeat = int(math.ceil(in_channels/1))\n",
    "    #conv1_weight = conv1_weight.repeat(1, repeat, 1, 1)[:, :in_channels, :, :]\n",
    "    #conv1_weight = conv1_weight.to(conv1_type)\n",
    "    #base_model.features[0].weight = nn.Parameter(conv1_weight)\n",
    "\n",
    "    layers = list(base_model.children())[:-2]\n",
    "    self.encoder = nn.Sequential(*layers)\n",
    "\n",
    "    if hasattr(base_model, \"fc\"):\n",
    "      in_features = base_model.fc.in_features\n",
    "    else:\n",
    "      in_features = base_model.classifier.in_features\n",
    "    \n",
    "    #in_features = base_model.embeddings[0].in_features\n",
    "\n",
    "    self.fc1 = nn.Linear(in_features, in_features, bias = True )\n",
    "\n",
    "\n",
    "    # Flattening layer for VGG16\n",
    "    #self.flatten = nn.Flatten()\n",
    "\n",
    "    self.att_block = AttBlock(\n",
    "        in_features, num_classes, activation = 'sigmoid'\n",
    "    )\n",
    "\n",
    "\n",
    "    self.init_weight()\n",
    "\n",
    "  def init_weight(self):\n",
    "    init_bn(self.bn0)\n",
    "    init_layer(self.fc1)\n",
    "\n",
    "  def preprocess(self, input, mixup_lambda=config.mixup_lambda):\n",
    "    x = input # (batch_size, 3, time_steps, mel_bins)\n",
    "    frames_num = x.shape[2]\n",
    "\n",
    "    x = x.transpose(1, 3)\n",
    "    x = self.bn0(x)\n",
    "    x = x.transpose(1, 3)\n",
    "\n",
    "    if self.training:\n",
    "      x = self.spec_augmenter(x)\n",
    "\n",
    "    # Mixup on spectrogram\n",
    "    if self.training and mixup_lambda is not None:\n",
    "      x = do_mixup(x, mixup_lambda)\n",
    "\n",
    "    x = x.transpose(2,3) ## EX005 does this. not sure what it does, but I know in_channels for first layer of VGGish is 1\n",
    "\n",
    "    return x, frames_num\n",
    "    \n",
    "  # add config.mixup_lambda  \n",
    "  def forward(self, input, mixup_lambda = None):\n",
    "    x, frames_num = self.preprocess(input, mixup_lambda = mixup_lambda)\n",
    "\n",
    "\n",
    "    x = self.encoder(x)\n",
    "\n",
    "\n",
    "    x = torch.mean(x, dim=3)\n",
    "\n",
    "    x1 = F.max_pool1d(x, kernel_size = 3, stride = 1, padding = 1)\n",
    "    x2 = F.avg_pool1d(x, kernel_size = 3, stride = 1, padding = 1)\n",
    "    x = x1+x2\n",
    "\n",
    "   \n",
    "    \n",
    "\n",
    "    x = F.dropout(x, p=0.5, training=self.training)\n",
    "    x = x.transpose(1, 2)\n",
    "\n",
    "    # Flattening layer for VGG16\n",
    "    #x = self.flatten(x)\n",
    "    \n",
    "    x = F.relu_(self.fc1(x))\n",
    "    x = x.transpose(1, 2)\n",
    "    x = F.dropout(x, p=0.5, training=self.training)\n",
    "\n",
    "    (clipwise_output, norm_att, segmentwise_output) = self.att_block(x)\n",
    "    logit = torch.sum(norm_att * self.att_block.cla(x), dim=2)\n",
    "    segmentwise_logit = self.att_block.cla(x).transpose(1, 2)\n",
    "    segmentwise_output = segmentwise_output.transpose(1, 2)\n",
    "\n",
    "    interpolate_ratio = frames_num // segmentwise_output.size(1)\n",
    "\n",
    "    framewise_output = interpolate(segmentwise_output,\n",
    "                                       interpolate_ratio)\n",
    "    framewise_output = pad_framewise_output(framewise_output, frames_num)\n",
    "\n",
    "    framewise_logit = interpolate(segmentwise_logit, interpolate_ratio)\n",
    "    framewise_logit = pad_framewise_output(framewise_logit, frames_num)\n",
    "\n",
    "\n",
    "    output_dict = {\n",
    "        'framewise_output': framewise_output,\n",
    "        'clipwise_output': clipwise_output,\n",
    "        'logit': logit,\n",
    "        'framewise_logit': framewise_logit\n",
    "    }\n",
    "\n",
    "\n",
    "    return output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "invalid-harvey",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mel Spec Image Transforms\n",
    "mean = (0.485, 0.456, 0.406) # RGB\n",
    "std = (0.229, 0.224, 0.225) # RGB\n",
    "\n",
    "spec_transforms = {\n",
    "    'train' : A.Compose([\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            \n",
    "            A.OneOf([\n",
    "                A.Cutout(max_h_size=5, max_w_size=16),\n",
    "                A.CoarseDropout(max_holes=4),\n",
    "            ], p=0.5),\n",
    "            A.Normalize(mean, std),\n",
    "    ]),\n",
    "    'valid' : A.Compose([\n",
    "            A.Normalize(mean, std),\n",
    "    ]),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "passing-continuity",
   "metadata": {},
   "outputs": [],
   "source": [
    "class config:\n",
    "  #####################\n",
    "  # Global #\n",
    "  #####################\n",
    "  EXP_NO = 'EXP001'\n",
    "  seed = 1\n",
    "  epochs = 5\n",
    "  base_dir = 'C:/UCI MDS Spring Quarter 2022/CS 274P/Final Project/train_audio'\n",
    "  # device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "  \n",
    "\n",
    "  #####################\n",
    "  # Dataset # \n",
    "  #####################\n",
    "\n",
    "  duration = 5        \n",
    "  n_mels = 224\n",
    "  sample_rate = 32000\n",
    "  fmin = 500          # Min Frequency    most birds vocalize between 500Hz and 12.5kHz\n",
    "  fmax = 12500        # Max Frequency\n",
    "  n_fft = 2048        # length of fft window\n",
    "  hop_length = 512    # length of non-intersecting portion of window length\n",
    "\n",
    "  #####################\n",
    "  # Dataloader Params\n",
    "  #####################\n",
    "\n",
    "  loader_params = {\n",
    "      \"train\": {\n",
    "          \"batch_size\": 16, #64\n",
    "          \"num_workers\": 0,\n",
    "          \"shuffle\": True\n",
    "      },\n",
    "      \"valid\": {\n",
    "          \"batch_size\": 32, #128\n",
    "          \"num_workers\": 0,\n",
    "          \"shuffle\": False\n",
    "      }\n",
    "  }\n",
    "\n",
    "\n",
    " \n",
    "  #####################\n",
    "  # Model # \n",
    "  #####################\n",
    "\n",
    "  base_model_name = \"tf_efficientnet_b0_ns\" # We should see the different types of models\n",
    "  pooling = \"max\" # not sure why we need to set this yet\n",
    "  classes = os.listdir(base_dir)\n",
    "  num_classes = 152\n",
    "  EARLY_STOPPING = True\n",
    "  EVALUATION = 'AUC'\n",
    "  pretrained = False\n",
    "  in_channels = 3\n",
    "  folds = [0] # [0,1,2,3,4]\n",
    "  num_folds = 5\n",
    "  LR = 1e-3\n",
    "  mel_bins = 224\n",
    "  mixup_lambda = None\n",
    "  \n",
    "  img_size = 224\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "above-latitude",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab Mel Spectrograms\n",
    "def get_mel_spects(y):\n",
    "    melspec = librosa.feature.melspectrogram(y=y, sr=config.sample_rate, n_mels=config.n_mels, \n",
    "                                             fmin=config.fmin, fmax=config.fmax, n_fft=config.n_fft, hop_length=config.hop_length)\n",
    "    melspec = librosa.power_to_db(melspec).astype(np.float32)\n",
    "    return melspec\n",
    "\n",
    "# Converts to RGB inputs for Pretrained model\n",
    "def mono_to_color(X, eps=1e-6):\n",
    "    X = np.stack([X, X, X], axis=-1)\n",
    "\n",
    "    # Standardize\n",
    "    mean = X.mean()\n",
    "    std = X.std()\n",
    "    X = (X - mean) / (std + eps)\n",
    "\n",
    "    # Normalize to [0, 255]\n",
    "    _min, _max = X.min(), X.max()\n",
    "\n",
    "    if (_max - _min) > eps:\n",
    "        V = np.clip(X, _min, _max)\n",
    "        V = 255 * (V - _min) / (_max - _min)\n",
    "        V = V.astype(np.uint8)\n",
    "    else:\n",
    "        V = np.zeros_like(X, dtype=np.uint8)\n",
    "\n",
    "    return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "homeless-strand",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataset(torchdata.Dataset):\n",
    "    def __init__(self, df: pd.DataFrame, clip: np.ndarray):\n",
    "        self.df = df\n",
    "        # self.clip = clip\n",
    "        self.clip = np.concatenate([clip, clip, clip])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        SR = 32000\n",
    "        sample = self.df.loc[idx, :]\n",
    "        row_id = sample.row_id\n",
    "\n",
    "        end_seconds = int(sample.seconds)\n",
    "        start_seconds = int(end_seconds - 5)\n",
    "        \n",
    "        # end_index = int(SR * (end_seconds + (self.train_period - 5) / 2) + len(self.clip) // 3)\n",
    "        # start_index = int(SR * (start_seconds - (self.train_period - 5) / 2) + len(self.clip) // 3)\n",
    "        \n",
    "        # y = self.clip[start_index:end_index].astype(np.float32)\n",
    "        image = self.clip[SR*start_seconds:SR*end_seconds].astype(np.float32)\n",
    "        image = np.nan_to_num(image)\n",
    "        \n",
    "        image = get_mel_spects(image)\n",
    "        image = mono_to_color(image)\n",
    "        image = image.astype(np.uint8)\n",
    "\n",
    "        image = spec_transforms['valid'](image=image)['image'].T\n",
    "            \n",
    "        return {\n",
    "            \"image\": image,\n",
    "            \"row_id\": row_id,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suspended-passenger",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_paths = ['C:/UCI MDS Spring Quarter 2022/CS 274P/Final Project/Saved_Models/model.pth']\n",
    "\n",
    "models = []\n",
    "\n",
    "for p in model_paths:\n",
    "    model = BirdCLEFPretrain(\n",
    "        mel_bins = config.mel_bins,\n",
    "        pretrained = config.pretrained,\n",
    "        num_classes = config.num_classes,\n",
    "        in_channels = config.in_channels)\n",
    "    \n",
    "    # model.to(device)\n",
    "    model.load_state_dict(torch.load(p))\n",
    "    model.eval()\n",
    "    models.append(model)\n",
    "    \n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "still-logic",
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_SR = 32000\n",
    "TESTDIR = Path()\"C:/UCI MDS Spring Quarter 2022/CS 274P/Final Project/test_soundscapes/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "portable-parish",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_audios = list(TESTDIR.glob(\"*.ogg\"))\n",
    "sample_submission = pd.read_csv(\"C:/UCI MDS Spring Quarter 2022/CS 274P/Final Project/sample_submission.csv\")\n",
    "sample_submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stainless-cornwall",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_for_clip(test_df: pd.DataFrame, \n",
    "                        clip: np.ndarray, \n",
    "                        models, \n",
    "                        threshold=0.05, \n",
    "                        threshold_long=None):\n",
    "\n",
    "    dataset = TestDataset(df=test_df, \n",
    "                          clip=clip,)\n",
    "    loader = torchdata.DataLoader(dataset, batch_size=1, shuffle=False)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "#     [model.eval() for model in models]\n",
    "    prediction_dict = {}\n",
    "    for data in tqdm(loader):\n",
    "        row_id = data['row_id'][0]\n",
    "        image = data['image'].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            probas = []\n",
    "            probas_long = []\n",
    "            for model in models:\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    output = model(image)\n",
    "                probas.append(output['clipwise_output'].detach().cpu().numpy().reshape(-1))\n",
    "                # probas_long.append(clipwise_pred_long.detach().cpu().numpy().reshape(-1))\n",
    "            probas = np.array(probas)\n",
    "            # probas_long = np.array(probas_long)\n",
    "#             probas = np.array([model(image)[1].detach().cpu().numpy().reshape(-1) for model in models])\n",
    "        if threshold_long is None:\n",
    "            events = probas.mean(0) >= threshold\n",
    "        else:\n",
    "            events = ((probas.mean(0) >= threshold).astype(int) \\\n",
    "                      + (probas_long.mean(0) >= threshold_long).astype(int)) >= 2\n",
    "        labels = np.argwhere(events).reshape(-1).tolist()\n",
    "#         labels = labels[:2]\n",
    "        if len(labels) == 0:\n",
    "            prediction_dict[str(row_id)] = \"nocall\"\n",
    "        else:\n",
    "            labels_str_list = list(map(lambda x: config.classes[x], labels))\n",
    "            label_string = \" \".join(labels_str_list)\n",
    "            prediction_dict[str(row_id)] = label_string\n",
    "    return prediction_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "backed-petite",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(test_audios,\n",
    "               threshold=0.05, \n",
    "               threshold_long=None):\n",
    "    \n",
    "    # models = [model]\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    prediction_dicts = {}\n",
    "    for audio_path in test_audios:\n",
    "       # with timer(f\"Loading {str(audio_path)}\", logger):\n",
    "       #     clip, _ = sf.read(audio_path, always_2d=True)\n",
    "        clip, _ = librosa.load(audio_path)\n",
    "        clip = np.mean(clip, 1)\n",
    "            \n",
    "        seconds = []\n",
    "        row_ids = []\n",
    "        for second in range(5, 65, 5):\n",
    "            row_id = \"_\".join(audio_path.name.split(\".\")[:-1]) + f\"_{second}\"\n",
    "            seconds.append(second)\n",
    "            row_ids.append(row_id)\n",
    "        print(row_ids)\n",
    "        test_df = pd.DataFrame({\n",
    "            \"row_id\": row_ids,\n",
    "            \"seconds\": seconds\n",
    "        })\n",
    "        #with timer(f\"Prediction on {audio_path}\", logger):\n",
    "            prediction_dict = prediction_for_clip(test_df,\n",
    "                                                  clip=clip,\n",
    "                                                  models=models,\n",
    "                                                  threshold=threshold, threshold_long=threshold_long)\n",
    "#         row_id = list(prediction_dict.keys())\n",
    "#         birds = list(prediction_dict.values())\n",
    "#         prediction_df = pd.DataFrame({\n",
    "#             \"row_id\": row_id,\n",
    "#             \"birds\": birds\n",
    "#         })\n",
    "#         prediction_dfs.append(prediction_df)\n",
    "#     prediction_df = pd.concat(prediction_dfs, axis=0, sort=False).reset_index(drop=True)\n",
    "        prediction_dicts.update(prediction_dict)\n",
    "    return prediction_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stupid-idaho",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.05\n",
    "threshold_long = None # 0.05\n",
    "\n",
    "prediction_dicts = prediction(test_audios=all_audios,\n",
    "           threshold=threshold, \n",
    "           threshold_long=threshold_long)\n",
    "print(prediction_dicts)\n",
    "\n",
    "for i in range(len(sample_submission)):\n",
    "    sample = sample_submission.row_id[i]\n",
    "    key = sample.split(\"_\")[0] + \"_\" + sample.split(\"_\")[1] + \"_\" + sample.split(\"_\")[3]\n",
    "    target_bird = sample.split(\"_\")[2]\n",
    "    print(key, target_bird)\n",
    "    if key in prediction_dicts:\n",
    "        sample_submission.iat[i, 1] = (target_bird in prediction_dicts[key])\n",
    "sample_submission.to_csv(\"submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
